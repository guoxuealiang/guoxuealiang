[{"id":"767796bded93cd64e5123ae500c1e849","title":"定时任务工具","content":"","slug":"定时任务工具","date":"2022-11-29T05:46:12.000Z","categories_index":"","tags_index":"","author_index":"Aurora"},{"id":"2a6d27eb5b0cfe90d21f2867e5b05654","title":"Es原理与使用","content":"1、ElasticSearch基本使用1.1、基本介绍1.1.1、ElasticSearch的特色ElasticSearch是实时的分布式搜索分析引擎，内部使用Lucene做索引与搜索\n\n准实时性：新增到ES中的数据在1秒后就可以被检索到，这种新增数据对搜索的可见性称为”准实时搜索”\n分布式：意味着可以动态调整集群规模，弹性扩容\n集群规模：可以扩展到上百台服务器，处理PB级结构化或非结构化数据\n各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其工作\n\nLucene是Java语言编写的全文搜索框架，用于处理纯文本的数据，但它只是一个库，提供建立索引、执行搜索等接口，但不包含分布式服务，这些正是 ES 做的\n1.1.2 ElasticSearch使用场景ElasticSearch广泛应用于各行业领域， 比如维基百科， GitHub的代码搜索，电商网站的大数据日志统计分析， BI系统报表统计分析等。\n\n提供分布式的搜索引擎和数据分析引擎\n比如百度，网站的站内搜索，IT系统的检索， 数据分析比如热点词统计， 电商网站商品TOP排名等。\n\n全文检索，结构化检索，数据分析\n支持全文检索， 比如查找包含指定名称的商品信息； 支持结构检索， 比如查找某个分类下的所有商品信息； \n还可以支持高级数据分析， 比如统计某个商品的点击次数， 某个商品有多少用户购买等等。\n\n支持海量数据准实时的处理\n采用分布式节点， 将数据分散到多台服务器上去存储和检索， 实现海量数据的处理， 比如统计用户的行为日志， 能够在秒级别对数据进行检索和分析。\n\n\n1.1.3 ElasticSearch基本概念介绍\n\n\nElasticSearch\nRelational Database\n\n\n\nIndex\nDatabase\n\n\nType\nTable\n\n\nDocument\nRow\n\n\nField\nColumn\n\n\nMapping\nSchema\n\n\nEverything is indexed\nIndex\n\n\nQuery DSL\nSQL\n\n\nGET http://…\nSELECT * FROM table…\n\n\nPUT http://…\nUPDATE table SET…\n\n\n\n\n索引（Index)\n相比传统的关系型数据库，索引相当于SQL中的一个【数据库】，或者一个数据存储方案(schema)。\n\n类型（Type）\n一个索引内部可以定义一个或多个类型， 在传统关系数据库来说， 类型相当于【表】的概念。\n\n文档（Document）\n文档是Lucene索引和搜索的原子单位，它是包含了一个或多个域的容器，采用JSON格式表示。相当于传统数据库【行】概念\n\n集群（Cluster）\n集群是由一台及以上主机节点组成并提供存储及搜索服务， 多节点组成的集群拥有冗余能力，它可以在一个或几个节点出现故障时保证服务的整体可用性。\n\n节点（Node）\nNode为集群中的单台节点，其可以为master节点亦可为slave节点（节点属性由集群内部选举得出）并提供存储相关数据的功能\n\n切片（shards)  分片\n切片是把一个大文件分割成多个小文件然后分散存储在集群中的多个节点上， 可以将其看作mysql的分库分表概念。 Shard有两种类型：primary主片和replica副本，primary用于文档存储，Replica shard是Primary Shard的副本，用于冗余数据及提高搜索性能。\n\n\n\n注意： ES7之后Type被舍弃，只有Index(等同于数据库+表定义）和Document（文档，行记录）。\n1.2 ElasticSearch安装1.2.1 下载ElasticSearch服务下载最新版ElasticSearch7.10.2： https://www.elastic.co/cn/start\n1.2.2 解压安装包tar -xvf elasticsearch-7.10.2-linux-x86_64.tar.gz\n\n1.2.3 创建elsearch用户## ElasticSearch不能以Root身份运行， 需要单独创建一个用户， 并赋予目录权限\ngroupadd elsearch\n\nuseradd elsearch -g elsearch -p elasticsearch\n\nchown -R elsearch:elsearch  &#x2F;opt&#x2F;elasticsearch&#x2F;elasticsearch-7.10.2\n\n1.2.4 修改配置文件vi config/elasticsearch.yml,  默认情况下会绑定本机地址， 外网不能访问， 这里要修改下：\n# node名称\nnode.name: node-1\n# 外网访问地址\nnetwork.host: 0.0.0.0\ndiscovery.seed_hosts: [&quot;node-1&quot;]\ncluster.initial_master_nodes: [&quot;node-1&quot;]\n\n1.2.5 关闭防火墙systemctl stop  firewalld.service\nsystemctl disable  firewalld.service\n\n1.2.6 JDK环境变量的配置1）最新版的ElasticSearch需要JDK11版本， 下载JDK11压缩包， 并进行解压。\n2）修改环境配置文件\nvi bin/elasticsearch-env ，参照以下位置， 追加一行， 设置JAVA_HOME， 指定JDK11路径。\nJAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk-11.0.11\n\n# now set the path to java\nif [ ! -z &quot;$JAVA_HOME&quot; ]; then\n  JAVA&#x3D;&quot;$JAVA_HOME&#x2F;bin&#x2F;java&quot;\nelse\n  if [ &quot;$(uname -s)&quot; &#x3D; &quot;Darwin&quot; ]; then\n    # OSX has a different structure\n    JAVA&#x3D;&quot;$ES_HOME&#x2F;jdk&#x2F;Contents&#x2F;Home&#x2F;bin&#x2F;java&quot;\n  else\n    JAVA&#x3D;&quot;$ES_HOME&#x2F;jdk&#x2F;bin&#x2F;java&quot;\n  fi\nfi\n\n1.2.7 启动ElasticSearch## 切换用户\nsu elsearch\n## 以后台常驻方式启动\nbin&#x2F;elasticsearch -d \n\nPS: 出现max virtual memory areas vm.max_map_count [65530] is too low, increase to at least 错误信息，如图：\n\n修改系统配置：\nvi /etc/sysctl.conf , 添加 vm.max_map_count=655360，保存退出后，执行 sysctl -p，使之生效。\nvm.max_map_count&#x3D;655360\n\nvi /etc/security/limits.conf，在文件末尾添加如下配置：\n* soft nofile 65536\n\n* hard nofile 131072\n\n* soft nproc 2048\n\n* hard nproc 4096\n\nelsearch soft nproc 125535\n\nelsearch hard nproc 125535\n\n1.2.8 访问验证访问地址：http://ip:9200/_cat/health\n启动状态有green、yellow和red。 green是代表启动正常。\n\n1.3 Kibana服务安装Kibana是一个针对Elasticsearch的开源分析及可视化平台，用来搜索、查看交互存储在Elasticsearch索引中的数据。\n1.3.1 下载安装包到官网下载， Kibana安装包,  与之对应7.10.2版本， 选择Linux 64位版本下载，并进行解压。\n# 解压\ntar -xvf kibana-7.10.2-linux-x86_64.tar.gz\n# 重命名为 kibana-7.10.2\nmv kibana-7.10.2-linux-x86_64&#x2F; kibana-7.10.2\n\n1.3.2 创建的elsearch用户Kibana启动不能使用root用户， 使用上面创建的elsearch用户， 进行赋权：\nchown -R elsearch:elsearch kibana-7.10.2\n\n1.3.3 修改配置文件vi config/kibana.yml , 修改以下配置：\n# 服务端口\nserver.port: 5601\n# 服务地址\nserver.host: &quot;0.0.0.0&quot;\n# elasticsearch服务地址\nelasticsearch.hosts: [&quot;http:&#x2F;&#x2F;192.168.10.30:9200&quot;]\n\n1.3.4 启动kibana.&#x2F;kibana -q\n\n看到以下日志， 代表启动正常\nlog   [01:40:00.143] [info][listening] Server running at http:&#x2F;&#x2F;0.0.0.0:5601\n\n如果出现启动失败的情况， 要检查集群各节点的日志， 确保服务正常运行状态。\n1.3.5 访问服务http://ip:5601/app/home#/\n1.4 ES的基础操作1.4.1 登录Kibana管理后台地址：  http://ip:5601\n进入”Dev Tools”栏，在Console中输入命令进行操作：\n1.4.2 索引新建索引 orders\n## 创建索引\nPUT orders\n\n2）查询索引 orders\n## 查询索引\nGET orders\n\n3）删除索引 orders\n## 删除索引\nDELETE orders\n\n4）索引的设置\n## 设置索引\nPUT orders\n&#123;\n  &quot;settings&quot;: &#123;\n    &quot;index&quot;: &#123;\n      &quot;number_of_shards&quot;: 1, \n      &quot;number_of_replicas&quot;: 0\n    &#125;\n  &#125;\n&#125;\n\n1.4.3 文档1）创建文档\n## 创建文档，生成默认的文档id\nPOST orders&#x2F;_doc\n&#123;\n  &quot;name&quot;: &quot;袜子1双&quot;,\n  &quot;price&quot;: &quot;200&quot;,\n  &quot;count&quot;: 1,\n  &quot;address&quot;: &quot;北京市&quot;\n&#125;\n## 创建文档，生成自定义文档id\nPOST orders&#x2F;_doc&#x2F;1\n&#123;\n  &quot;name&quot;: &quot;袜子1双&quot;,\n  &quot;price&quot;: &quot;2&quot;,\n  &quot;count&quot;: 1,\n  &quot;address&quot;: &quot;北京市&quot;\n&#125;\n\n2）查询文档\n## 根据指定的id查询\nGET orders&#x2F;_doc&#x2F;1\n## 根据指定条件查询文档\nGET orders&#x2F;_search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;match&quot;: &#123;\n      &quot;address&quot;: &quot;北京市&quot;\n    &#125;\n  &#125;\n&#125;\n## 查询全部文档\nGET orders&#x2F;_search\n\n3） 更新文档\n## 更新文档 覆盖\nPOST orders&#x2F;_doc&#x2F;1\n&#123;\n  &quot;price&quot;: &quot;200&quot;\n&#125;\n## 更新文档 跟新指定字段值\nPOST orders&#x2F;_update&#x2F;1\n&#123;\n  &quot;doc&quot;: &#123;\n    &quot;price&quot;: &quot;200&quot;\n  &#125;\n&#125;\n\n4）删除文档\n## 删除文档\nDELETE orders&#x2F;_doc&#x2F;1\n\n1.4.4 映射对于映射，只能进行字段添加，不能对字段进行修改或删除，如有需要，则重新创建映射。\n## 设置mapping信息\nPUT orders&#x2F;_mappings\n&#123;\n  &quot;properties&quot;:&#123;\n    &quot;price&quot;: &#123;\n      &quot;type&quot;: &quot;long&quot;\n    &#125;\n  &#125;\n&#125;\n## 设置分片和映射\nPUT orders\n&#123;\n  &quot;settings&quot;: &#123;\n    &quot;index&quot;: &#123;\n      &quot;number_of_shards&quot;: 1, \n      &quot;number_of_replicas&quot;: 0\n    &#125;\n  &#125;,\n  &quot;mappings&quot;: &#123;\n    &quot;properties&quot;: &#123;\n      &quot;name&quot;: &#123;\n        &quot;type&quot;: &quot;text&quot;\n      &#125;,\n      &quot;price&quot;: &#123;\n        &quot;type&quot;: &quot;long&quot;\n      &#125;,\n      &quot;count&quot;: &#123;\n        &quot;type&quot;: &quot;long&quot;\n      &#125;,\n      &quot;address&quot;: &#123;\n        &quot;type&quot;: &quot;text&quot;\n      &#125;\n    &#125;\n  &#125;\n&#125;\n\n\n\n1.5 ES数据类型1.5.1 整体数据类型结构\n1.5.3 Date时间类型数据库里的日期类型需要规范具体的传入格式， ES是可以控制，自适应处理。\n## ES的Date类型允许可以使用的格式：yyyy-MM-dd\nPUT my_date_index&#x2F;_doc&#x2F;1\n&#123;\n    &quot;date&quot;: &quot;2021-01-01&quot;\n&#125; \n## ES的Date类型允许可以使用的格式：yyyy-MM-dd HH:mm:ss\nPUT my_date_index&#x2F;_doc&#x2F;2\n&#123;\n    &quot;date&quot;: &quot;2021-01-01T12:10:30Z&quot;\n&#125; \n## ES的Date类型允许可以使用的格式：epoch_millis(毫秒值)\nPUT my_date_index&#x2F;_doc&#x2F;3\n&#123;\n    &quot;date&quot;: 1520071600001\n&#125; \n\n\n## 查看日期数据：\nGET my_date_index&#x2F;_mapping\n\n1.5.4 复合类型1）数组\n在Elasticsearch中，数组不需要声明专用的字段数据类型。但是，在数组中的所有值都必须具有相同的数据类型。\n## 错误示例\nPOST orders&#x2F;_doc&#x2F;1\n&#123;\n  &quot;goodsName&quot;:[&quot;足球&quot;,&quot;篮球&quot;,&quot;兵乓球&quot;, 3]\n&#125;\n## 正确示例\nPOST orders&#x2F;_doc&#x2F;1\n&#123;\n  &quot;goodsName&quot;:[&quot;足球&quot;,&quot;篮球&quot;,&quot;兵乓球&quot;]\n&#125;\n\n2）对象\n用于存储单个JSON对象， 类似于JAVA中的对象类型， 可以有多个值， 比如LIST，可以包含多个对象。但是，LIST&lt; Object &gt;只能作为整体， 不能独立的索引查询。\n# 新增第一组数据， 组别为美国，两个人。\nPOST my_index&#x2F;_doc&#x2F;1\n&#123;\n  &quot;group&quot; : &quot;america&quot;,  \n  &quot;users&quot; : [   \n    &#123;   \n      &quot;name&quot; : &quot;John&quot;,  \n      &quot;age&quot; :  &quot;22&quot; \n    &#125;,  \n    &#123;   \n      &quot;name&quot; : &quot;Alice&quot;, \n      &quot;age&quot; :  &quot;21&quot; \n    &#125;   \n  ]\n&#125;   \n# 新增第二组数据， 组别为英国， 两个人。\nPOST my_index&#x2F;_doc&#x2F;2\n&#123;   \n  &quot;group&quot; : &quot;england&quot;,  \n  &quot;users&quot; : [   \n    &#123;   \n      &quot;name&quot; : &quot;lucy&quot;,  \n      &quot;age&quot; :  &quot;21&quot; \n    &#125;,  \n    &#123;   \n      &quot;name&quot; : &quot;John&quot;,  \n      &quot;age&quot; :  &quot;32&quot; \n    &#125;   \n  ] \n&#125;\n## 搜索name为John，age为21的数据\nGET my_index&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;bool&quot;: &#123;\n            &quot;must&quot;: [\n                &#123;\n                    &quot;match&quot;: &#123;\n                        &quot;users.name&quot;: &quot;John&quot;\n                    &#125;\n                &#125;,\n                &#123;\n                    &quot;match&quot;: &#123;\n                        &quot;users.age&quot;: &quot;21&quot;\n                    &#125;\n                &#125;\n            ]\n        &#125;\n    &#125;\n&#125;\n\n结果可以看到， 这两组数据都能找出，因为每一组数据都是作为一个整体进行搜索匹配， 而非具体某一条数据。\n3）嵌套\n用于存储多个JSON对象组成的数组，nested 类型是 object 类型中的一个特例，可以让对象数组独立索引和查询。\n## 1. 创建nested类型的索引\nPUT my_index\n&#123;   \n  &quot;mappings&quot;: &#123; \n    &quot;properties&quot;: &#123; \n      &quot;users&quot;: &#123;    \n        &quot;type&quot;: &quot;nested&quot;    \n      &#125; \n    &#125;   \n  &#125; \n&#125;\n## 2. 新增数据\n# 新增第一组数据， 组别为美国，两个人。\nPOST my_index&#x2F;_doc&#x2F;1\n&#123;\n  &quot;group&quot; : &quot;america&quot;,  \n  &quot;users&quot; : [   \n    &#123;   \n      &quot;name&quot; : &quot;John&quot;,  \n      &quot;age&quot; :  &quot;22&quot; \n    &#125;,  \n    &#123;   \n      &quot;name&quot; : &quot;Alice&quot;, \n      &quot;age&quot; :  &quot;21&quot; \n    &#125;   \n  ]\n&#125;   \n# 新增第二组数据， 组别为英国， 两个人。\nPOST my_index&#x2F;_doc&#x2F;2\n&#123;   \n  &quot;group&quot; : &quot;england&quot;,  \n  &quot;users&quot; : [   \n    &#123;   \n      &quot;name&quot; : &quot;lucy&quot;,  \n      &quot;age&quot; :  &quot;21&quot; \n    &#125;,  \n    &#123;   \n      &quot;name&quot; : &quot;John&quot;,  \n      &quot;age&quot; :  &quot;32&quot; \n    &#125;   \n  ] \n&#125;\n## 3. 再次查询\nGET my_index&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;bool&quot;: &#123;\n            &quot;must&quot;: [\n                &#123;\n                    &quot;nested&quot;: &#123;\n                        &quot;path&quot;: &quot;users&quot;,\n                        &quot;query&quot;: &#123;\n                            &quot;bool&quot;: &#123;\n                                &quot;must&quot;: [\n                                    &#123;\n                                        &quot;match&quot;: &#123;\n                                            &quot;users.name&quot;: &quot;John&quot;\n                                        &#125;\n                                    &#125;,\n                                    &#123;\n                                        &quot;match&quot;: &#123;\n                                            &quot;users.age&quot;: &quot;21&quot;\n                                        &#125;\n                                    &#125;\n                                ]\n                            &#125;\n                        &#125;\n                    &#125;\n                &#125;\n            ]\n        &#125;\n    &#125;\n&#125;\n\n采用以前的条件， 这个时候查不到任何结果， 将年龄改成22， 就可以找出对应的数据\n1.5.5 GEO地理位置类型现在大部分APP都有基于位置搜索的功能， 比如交友、购物应用等。这些功能是基于GEO搜索实现的。\n对于GEO地理位置类型，分为地理坐标类型：Geo-point， 和形状：Geo-shape 两种类型。\n\n\n\n经纬度\n英文\n简写\n正数\n负数\n\n\n\n纬度\nlatitude\nlat\n北纬\n南纬\n\n\n经度\nlongitude\nlon或lng\n东经\n西经\n\n\n1）创建地理位置索引\nPUT my_locations\n&#123;\n  &quot;mappings&quot;: &#123;\n    &quot;properties&quot;: &#123;\n      &quot;location&quot;: &#123;\n        &quot;type&quot;: &quot;geo_point&quot;\n      &#125;\n    &#125;\n  &#125;\n&#125;\n\n2）添加地理位置数据\n# 采用object对象类型\nPUT my_locations&#x2F;_doc&#x2F;1\n&#123;\n    &quot;user&quot;: &quot;张三&quot;,\n    &quot;text&quot;: &quot;Geo-point as an object&quot;,\n    &quot;location&quot;: &#123;\n        &quot;lat&quot;: 41.12,\n        &quot;lon&quot;: -71.34\n    &#125;\n&#125;\n# 采用string类型\nPUT my_locations&#x2F;_doc&#x2F;2\n&#123;\n    &quot;user&quot;: &quot;李四&quot;,\n    &quot;text&quot;: &quot;Geo-point as a string&quot;,\n    &quot;location&quot;: &quot;45.12,-75.34&quot;\n&#125;\n# 采用geohash类型（geohash算法可以将多维数据映射为一串字符）\nPUT my_locations&#x2F;_doc&#x2F;3\n&#123;\n    &quot;user&quot;: &quot;王二麻子&quot;,\n    &quot;text&quot;: &quot;Geo-point as a geohash&quot;,\n    &quot;location&quot;: &quot;drm3btev3e86&quot;\n&#125;\n# 采用array数组类型\nPUT my_locations&#x2F;_doc&#x2F;4\n&#123;\n    &quot;user&quot;: &quot;木头老七&quot;,\n    &quot;text&quot;: &quot;Geo-point as an array&quot;,\n    &quot;location&quot;: [\n        -80.34,\n        51.12\n    ]\n&#125;\n\n3）需求\n搜索出距离我{“lat” : 40,”lon” : -70} 200km范围内的人\nGET my_locations&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;bool&quot;: &#123;\n            &quot;must&quot;: &#123;\n                &quot;match_all&quot;: &#123;&#125;\n            &#125;,\n            &quot;filter&quot;: &#123;\n                &quot;geo_distance&quot;: &#123;\n                    &quot;distance&quot;: &quot;200km&quot;,\n                    &quot;location&quot;: &#123;\n                        &quot;lat&quot;: 40,\n                        &quot;lon&quot;: -70\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n2. ES高可用集群配置2.1 集群介绍2.1.1 主节点（候选主节点）\n主节点负责创建索引、删除索引、分配分片、追踪集群中的节点状态等工作， 主节点负荷相对较轻， 客户端请求可以直接发往任何节点， 由对应节点负责分发和返回处理结果。\n​    一个节点启动之后， 采用 Zen Discovery机制去寻找集群中的其他节点， 并与之建立连接， 集群会从候选主节点中选举出一个主节点， 并且一个集群只能选举一个主节点， 在某些情况下， 由于网络通信丢包等问题， 一个集群可能会出现多个主节点， 称为“脑裂现象”， 脑裂会存在丢失数据的可能， 因为主节点拥有最高权限， 它决定了什么时候可以创建索引， 分片如何移动等， 如果存在多个主节点， 就会产生冲突， 容易产生数据丢失。要尽量避免这个问题， 可以通过 discovery.zen.minimum_master_nodes 来设置最少可工作的候选主节点个数。  建议设置为（候选主节点/2） + 1 比如三个候选主节点，该配置项为 （3/2）+1 ,来保证集群中有半数以上的候选主节点， 没有足够的master候选节点， 就不会进行master节点选举，减少脑裂的可能。\n主节点的参数设置：\nnode.master &#x3D; true\nnode.data &#x3D; false\n\n2.1.2 数据节点\n数据节点负责数据的存储和CRUD等具体操作，数据节点对机器配置要求比较高，首先需要有足够的磁盘空间来存储数据，其次数据操作对系统CPU、Memory和IO的性能消耗都很大。通常随着集群的扩大，需要增加更多的数据节点来提高可用性。\n数据节点的参数设置：\nnode.master &#x3D; false\nnode.data &#x3D; true\n\n2.1.3 客户端节点\n客户端节点不做候选主节点， 也不做数据节点的节点，只负责请求的分发、汇总等等，增加客户端节点类型更多是为了负载均衡的处理。\n客户端节点的参数设置：\nnode.master &#x3D; false\nnode.data &#x3D; false\n\n2.1.4 提取节点（预处理）\n能执行预处理管道，有自己独立的任务要执行， 在索引数据之前可以先对数据做预处理操作， 不负责数据存储也不负责集群相关的事务。\n参数设置：\nnode.ingest &#x3D; true\n\n2.1.5 协调节点\n协调节点，是一种角色，而不是真实的Elasticsearch的节点，不能通过配置项来指定哪个节点为协调节点。集群中的任何节点，都可以充当协调节点的角色。当一个节点A收到用户的查询请求后，会把查询子句分发到其它的节点，然后合并各个节点返回的查询结果，最后返回一个完整的数据集给用户。在这个过程中，节点A扮演的就是协调节点的角色。\n ES的一次请求非常类似于Map-Reduce操作。在ES中对应的也是两个阶段，称之为scatter-gather。客户端发出一个请求到集群的任意一个节点，这个节点就是所谓的协调节点，它会把请求转发给含有相关数据的节点(scatter阶段)，这些数据节点会在本地执行请求然后把结果返回给协调节点。协调节点将这些结果汇总(reduce)成一个单一的全局结果集(gather阶段) 。\n2.1.6 部落节点\n在多个集群之间充当联合客户端，   它是一个特殊的客户端 ， 可以连接多个集群，在所有连接的集群上执行搜索和其他操作。  部落节点从所有连接的集群中检索集群状态并将其合并成全局集群状态。 掌握这一信息，就可以对所有集群中的节点执行读写操作，就好像它们是本地的。 请注意，部落节点需要能够连接到每个配置的集群中的每个单个节点。 \n2.2 集群原理2.2.1 集群分布式原理集群可以根据节点数， 动态调整主分片与副本数， 做到整个集群有效均衡负载。\n单节点状态下：\n\n两个节点状态下， 副本数为1：\n\n三个节点状态下， 副本数为1：\n\n三个节点状态下， 副本数为2：\n\n2.2.2 分片处理机制设置分片大小的时候， 需预先做好容量规划， 如果节点数过多， 分片数过少， 那么新的节点将无法分片， 不能做到水平扩展， 并且单个分片数据量太大， 导致数据重新分配耗时过长。\n 假设一个集群中有一个主节点、两个数据节点。orders索引的分片分布情况如下所示：\nPUT orders\n&#123;\n    &quot;settings&quot;:&#123;\n        &quot;number_of_shards&quot;: 2,  ## 主分片一共 2\n        &quot;number_of_replicas&quot;: 2  ## 副分片一共 4\n    &#125;\n&#125;\n\n整个集群中存在P0和P1两个主分片， P0对应的两个R0副本分片， P1对应的是两个R1副本分片。\n\n2.2.3 写索引处理流程1）客户端向NODE1发送写请求。\n2）NODE1使用文档ID来确定文档属于分片0，通过集群状态中的内容路由表信息获知分片0的主分片位于NODE3，因此请求被转发到NODE3上。\n3）NODE3上的主分片执行写操作。如果写入成功，则它将请求并行转发到 NODE1和NODE2的副分片上，等待返回结果。当所有的副分片都报告成功，NODE3将向协调节点报告成功，协调节点再向客户端报告成功。\n\n2.2.4 读取索引处理流程1）客户端向NODE1发送读请求。\n2）NODE1使用文档ID来确定文档属于分片0，通过集群状态中的内容路由表信息获知分片0有三个副本数据，位于所有的三个节点中，此时它可以将请求发送到任意节点，这里它将请求转发到NODE2。\n3）NODE2将文档返回给 NODE1,NODE1将文档返回给客户端。\nNODE1作为协调节点，会将客户端请求轮询发送到集群的所有副本来实现负载均衡。\n\n2.3 集群部署规划准备一台虚拟机：\nip： node1 （节点一）,  端口：9200， 9300\nip： node2 （节点二），端口：9201， 9301\nip： node3 （节点三），端口：9202， 9302\n2.4 集群配置2.4.1 下载安装包下载最新版ElasticSearch7.10.2： https://www.elastic.co/cn/start\n2.4.2 解压安装包将安装包解压至/opt/elasticsearch/cluster目录\nmkdir -p &#x2F;opt&#x2F;elasticsearch&#x2F;cluster\ncd &#x2F;opt&#x2F;elasticsearch&#x2F;cluster\ntar -xvf elasticsearch-7.10.2-linux-x86_64.tar.gz\n\n2.4.3 修改集群配置文件vi node1/config/elasticsearch.yml\n# 集群名称\ncluster.name: my-application\n#节点名称\nnode.name: node-1\n# 绑定IP地址\nnetwork.host: 192.168.10.30\n# 指定服务访问端口\nhttp.port: 9200\n# 指定API端户端调用端口\ntransport.tcp.port: 9300\n#集群通讯地址\ndiscovery.seed_hosts: [&quot;192.168.10.30:9300&quot;, &quot;192.168.10.30:9301&quot;,&quot;192.168.10.30:9302&quot;]\n#集群初始化能够参选的节点信息\ncluster.initial_master_nodes: [&quot;192.168.10.30:9300&quot;, &quot;192.168.10.30:9301&quot;,&quot;192.168.10.30:9302&quot;]\n#开启跨域访问支持，默认为false\nhttp.cors.enabled: true\n##跨域访问允许的域名, 允许所有域名\nhttp.cors.allow-origin: &quot;*&quot;\n\n\n修改目录权限\nchown -R elsearch:elsearch &#x2F;opt&#x2F;elasticsearch&#x2F;cluster&#x2F;node1\n\n2.4.4 创建其余两个节点复制node1安装目录：\ncd &#x2F;opt&#x2F;elasticsearch&#x2F;cluster\ncp -r node1 node2\ncp -r node1 node3\n\n2.4.5 修改其余节点的配置1）node2节点配置内容\n# 集群名称\ncluster.name: my-application\n#节点名称\nnode.name: node-2\n# 绑定IP地址\nnetwork.host: 192.168.10.30\n# 指定服务访问端口\nhttp.port: 9201\n# 指定API端户端调用端口\ntransport.tcp.port: 9301\n#集群通讯地址\ndiscovery.seed_hosts: [&quot;192.168.10.30:9300&quot;, &quot;192.168.10.30:9301&quot;,&quot;192.168.10.30:9302&quot;]\n#集群初始化能够参选的节点信息\ncluster.initial_master_nodes: [&quot;192.168.10.30:9300&quot;, &quot;192.168.10.30:9301&quot;,&quot;192.168.10.30:9302&quot;]\n#开启跨域访问支持，默认为false\nhttp.cors.enabled: true\n##跨域访问允许的域名, 允许所有域名\nhttp.cors.allow-origin: &quot;*&quot;\n\n\n2）node3节点配置内容\n# 集群名称\ncluster.name: my-application\n#节点名称\nnode.name: node-3\n# 绑定IP地址\nnetwork.host: 192.168.10.30\n# 指定服务访问端口\nhttp.port: 9202\n# 指定API端户端调用端口\ntransport.tcp.port: 9302\n#集群通讯地址\ndiscovery.seed_hosts: [&quot;192.168.10.30:9300&quot;, &quot;192.168.10.30:9301&quot;,&quot;192.168.10.30:9302&quot;]\n#集群初始化能够参选的节点信息\ncluster.initial_master_nodes: [&quot;192.168.10.30:9300&quot;, &quot;192.168.10.30:9301&quot;,&quot;192.168.10.30:9302&quot;]\n#开启跨域访问支持，默认为false\nhttp.cors.enabled: true\n##跨域访问允许的域名, 允许所有域名\nhttp.cors.allow-origin: &quot;*&quot;\n\n2.4.6 启动集群节点## 切换elsearch用户\nsu elsearch\n## 分别启动三个ES服务\n&#x2F;opt&#x2F;elasticsearch&#x2F;cluster&#x2F;node1&#x2F;bin&#x2F;elasticsearch -d\n&#x2F;opt&#x2F;elasticsearch&#x2F;cluster&#x2F;node2&#x2F;bin&#x2F;elasticsearch -d\n&#x2F;opt&#x2F;elasticsearch&#x2F;cluster&#x2F;node3&#x2F;bin&#x2F;elasticsearch -d\n\n注意： 如果启动出现错误， 将各节点的data目录清空， 再重启服务。\n2.4.7 集群状态查看http://192.168.10.30:9200/_cat/nodes?pretty\n可以看到三个节点信息，三个节点会自行选举出主节点。\n2.5 集群分片测试2.5.1 修改kibana配置elasticsearch.hosts: [&quot;http:&#x2F;&#x2F;192.168.10.30:9200&quot;,&quot;http:&#x2F;&#x2F;192.168.10.30:9201&quot;,&quot;http:&#x2F;&#x2F;192.168.10.30:9202&quot;]\n\n重启kibana服务， 进入控制台：http://192.168.10.30:5601/app/home#/\n2.5.2 设置分片数1）2个主分片、2个副分片\n## 再次创建索引（副本数量范围内）\nPUT orders\n&#123;\n    &quot;settings&quot;: &#123;\n        &quot;index&quot;: &#123;\n            &quot;number_of_shards&quot;: 2,\n            &quot;number_of_replicas&quot;: 2\n        &#125;\n    &#125;\n&#125;\n\n结果正常：\n\n2）2个主分片、5个副分片\nPUT orders\n&#123;\n    &quot;settings&quot;: &#123;\n        &quot;index&quot;: &#123;\n            &quot;number_of_shards&quot;: 2,\n            &quot;number_of_replicas&quot;: 5\n        &#125;\n    &#125;\n&#125;\n\nyellow警告错误：\n\n2.5.3 分片设置总结集群并非可以随意增加副本数量\n3. ELK部署应用与工作机制3.1 ELK日志分析平台介绍4. ElasticSearch高阶操作4.1 SpringBoot整合ES1）依赖的引入\n&lt;!-- elasticsearch--&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.elasticsearch.client&lt;&#x2F;groupId&gt;\n    &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;&#x2F;artifactId&gt;\n    &lt;version&gt;7.10.2&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.elasticsearch.client&lt;&#x2F;groupId&gt;\n    &lt;artifactId&gt;elasticsearch-rest-client&lt;&#x2F;artifactId&gt;\n    &lt;version&gt;7.10.2&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.elasticsearch&lt;&#x2F;groupId&gt;\n    &lt;artifactId&gt;elasticsearch&lt;&#x2F;artifactId&gt;\n    &lt;version&gt;7.10.2&lt;&#x2F;version&gt;\n&lt;&#x2F;dependency&gt;\n\n2）ESConfig连接配置类\n@Configuration\n@ConfigurationProperties(prefix &#x3D; &quot;elasticsearch&quot;)\npublic class ESConfig &#123;\n\n    private String host;\n\n    private int port;\n\n\n    public String getHost() &#123;\n        return host;\n    &#125;\n\n    public void setHost(String host) &#123;\n        this.host &#x3D; host;\n    &#125;\n\n    public int getPort() &#123;\n        return port;\n    &#125;\n\n    public void setPort(int port) &#123;\n        this.port &#x3D; port;\n    &#125;\n\n    &#x2F;**\n     * es java客户端\n     *\n     * @return\n     *&#x2F;\n    @Bean\n    public RestHighLevelClient restHighLevelClient() &#123;\n        RestClientBuilder builder &#x3D; RestClient.builder(new HttpHost(host, port, &quot;http&quot;));\n        builder.setRequestConfigCallback(requestConfigBuilder -&gt; &#123;\n            requestConfigBuilder.setConnectionRequestTimeout(500000);\n            requestConfigBuilder.setSocketTimeout(500000);\n            requestConfigBuilder.setConnectTimeout(500000);\n            return requestConfigBuilder;\n        &#125;);\n        return new RestHighLevelClient(builder);\n    &#125;\n&#125;\n\n4.2 倒排索引4.2.1 概念要想理解倒排索引，首先先思考一个问题，获取某个文件夹下所有文件名中包含Spring的文件\n1）确定要搜索的文件夹\n2）遍历文件夹下所有文件\n3）判断文件名中是否包含Spring\n\n这种思维可以理解为是一种正向思维的方式，从外往内，根据key找value。这种方式可以理解为正向索引。\n4.2.2 结构而ElasticSearch为了提升查询效率，采用反向思维方式，根据value找key。\n\n4.3 IK分词器4.3.1 认识分词器查询出了很多万豪相关的酒店，现在以北京市东城区万豪酒店查询name域，可以发现无法查询到结果。\nGET hotel&#x2F;_search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;term&quot;: &#123;\n      &quot;name&quot;: &quot;北京市东城区万豪酒店&quot;\n    &#125;\n  &#125;\n&#125;\n\nMySQL [elasticsearch_db]&gt; select * from t_hotel where name &#x3D; &#39;北京市东城区万豪酒店&#39; \\G;]\n\n在创建索引时，对于name域，数据类型是text。当添加文档时，对于该域的值会进行分词，形成若干term（词条）存储在倒排索引中。\n根据倒排索引结构，当查询条件在词条中存在，则会查询到数据。如果词条中没有，则查询不到数据。\n那么对于北京市东城区万豪酒店的分词结果是什么呢？\nGET hotel&#x2F;_analyze\n&#123;\n &quot;field&quot;: &quot;name&quot;,  \n  &quot;text&quot;: &quot;北京市东城区万豪酒店&quot;\n&#125;\n\nGET hotel&#x2F;_analyze\n&#123;\n  &quot;field&quot;: &quot;name&quot;, \n  &quot;analyzer&quot;: &quot;standard&quot;,\n  &quot;text&quot;: &quot;北京市东城区万豪酒店&quot;\n&#125;\n\nGET _analyze\n&#123;\n  &quot;analyzer&quot;: &quot;ik_smart&quot;,\n  &quot;text&quot;: &quot;北京市东城区万豪酒店&quot;\n&#125;\n\nGET _analyze\n&#123;\n  &quot;analyzer&quot;: &quot;ik_max_word&quot;,\n  &quot;text&quot;: &quot;北京市东城区万豪酒店&quot;\n&#125;\n\n此时可以发现，每个字形成了一个词，所以并没有找到相匹配的词，导致无法查询到结果\n在ElasticSearch默认内置了多种分词器：\n\nStandard Analyzer - 默认分词器，按英文空格切分 \nSimple Analyzer - 按照非字母切分(符号被过滤)\nStop Analyzer - 小写处理，停用词过滤(the,a,is) \nWhitespace Analyzer - 按照空格切分，不转小写 \nKeyword Analyzer - 不分词，直接将输入当作输出 \nPatter Analyzer - 正则表达式，默认\\W+(非字符分割) \n\n而我们想要的是，分词器能够智能的将中文按照词义分成若干个有效的词。此时就需要额外安装中文分词器。 对于中文分词器的类型也有很多，其中首选的是：IK分词器。\n4.3.2 安装IK分词器1）安装IK分词插件\n下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.10.2/elasticsearch-analysis-ik-7.10.2.zip\n2）采用本地文件安装方式， 进入ES安装目录， 执行插件安装命令\n[root@linux30 cluster]# .&#x2F;node1&#x2F;bin&#x2F;elasticsearch-plugin install file:&#x2F;&#x2F;&#x2F;opt&#x2F;elasticsearch&#x2F;elasticsearch-analysis-ik-7.10.2.zip\n[root@linux30 cluster]# .&#x2F;node2&#x2F;bin&#x2F;elasticsearch-plugin install file:&#x2F;&#x2F;&#x2F;opt&#x2F;elasticsearch&#x2F;elasticsearch-analysis-ik-7.10.2.zip\n[root@linux30 cluster]# .&#x2F;node3&#x2F;bin&#x2F;elasticsearch-plugin install file:&#x2F;&#x2F;&#x2F;opt&#x2F;elasticsearch&#x2F;elasticsearch-analysis-ik-7.10.2.zip\n\n安装成功后， 会给出对应提示：\n-&gt; Installing file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;elasticsearch-7.10.2&#x2F;elasticsearch-analysis-ik-7.10.2.zip\n-&gt; Downloading file:&#x2F;&#x2F;&#x2F;usr&#x2F;local&#x2F;elasticsearch-7.10.2&#x2F;elasticsearch-analysis-ik-7.10.2.zip\n[&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] 100%   \n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@     WARNING: plugin requires additional permissions     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n* java.net.SocketPermission * connect,resolve\nSee http:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;guides&#x2F;security&#x2F;permissions.html\nfor descriptions of what these permissions allow and the associated risks.\n\nContinue with installation? [y&#x2F;N]y\n-&gt; Installed analysis-ik\n\n3）重启ElasticSearch服务\n4.3.3 测试IK分词器## standard标准分词器\nGET _analyze\n&#123;\n  &quot;analyzer&quot;: &quot;standard&quot;,\n  &quot;text&quot;: &quot;北京市东城区万豪酒店&quot;\n&#125;\n## IK智能化分词器\nGET _analyze\n&#123;\n  &quot;analyzer&quot;: &quot;ik_smart&quot;,\n  &quot;text&quot;: &quot;北京市东城区万豪酒店&quot;\n&#125;\n## ik_max_word最大化分词\nGET _analyze\n&#123;\n  &quot;analyzer&quot;: &quot;ik_max_word&quot;,\n  &quot;text&quot;: &quot;北京市东城区万豪酒店&quot;\n&#125;\n\n4.3.4 IK分词器最佳运用analyzer指定的是构建索引的分词，search_analyzer指定的是搜索关键字的分词。\n实践运用的时候， 构建索引的时候采用max_word，将分词最大化； 查询的时候则使用smartword智能化分词，这样能够最大程度的匹配出结果。\nPUT hotel\n&#123;\n  &quot;settings&quot;: &#123;\n    &quot;number_of_shards&quot;: 1,\n    &quot;number_of_replicas&quot;: 0\n  &#125;, \n  &quot;mappings&quot;: &#123;\n    &quot;properties&quot;: &#123;\n      \n      &quot;name&quot;:&#123;\n        &quot;type&quot;: &quot;text&quot;,\n        &quot;analyzer&quot;: &quot;ik_max_word&quot;,\n        &quot;search_analyzer&quot;:&quot;ik_smart&quot;\n      &#125;,\n      &quot;address&quot;:&#123;\n        &quot;type&quot;: &quot;text&quot;,\n        &quot;analyzer&quot;: &quot;ik_max_word&quot;\n      &#125;,\n      &quot;brand&quot;:&#123;\n        &quot;type&quot;: &quot;keyword&quot;\n      &#125;,\n      &quot;type&quot;:&#123;\n        &quot;type&quot;: &quot;keyword&quot;\n      &#125;,\n       &quot;price&quot;:&#123;\n        &quot;type&quot;: &quot;integer&quot;\n      &#125;,\n      &quot;specs&quot;:&#123;\n        &quot;type&quot;: &quot;keyword&quot;\n      &#125;,\n       &quot;salesVolume&quot;:&#123;\n        &quot;type&quot;: &quot;integer&quot;\n      &#125;,\n      &quot;area&quot;:&#123;\n        &quot;type&quot;: &quot;text&quot;,\n        &quot;analyzer&quot;: &quot;ik_max_word&quot;,\n        &quot;search_analyzer&quot;:&quot;ik_smart&quot;\n      &#125;,\n      &quot;imageUrl&quot;:&#123;\n        &quot;type&quot;: &quot;text&quot;\n      &#125;,\n      &quot;synopsis&quot;:&#123;\n        &quot;type&quot;: &quot;text&quot;,\n        &quot;analyzer&quot;: &quot;ik_max_word&quot;\n      &#125;,\n      &quot;createTime&quot;:&#123;\n        &quot;type&quot;: &quot;date&quot;,\n        &quot;format&quot;: &quot;yyyy-MM-dd&quot;\n      &#125;,\n      &quot;isAd&quot;:&#123;\n        &quot;type&quot;:&quot;integer&quot;\n      &#125;\n    &#125;\n  &#125;\n&#125;\n\n4.3.5 自定义扩展词、停用词1）编辑 IKAnalyzer.cfg.xml文件，配置自定义扩展词 my_ext.dic 和 停用词 my_stopword.dic。\n[root@linux30 cluster]# cat node1&#x2F;config&#x2F;analysis-ik&#x2F;IKAnalyzer.cfg.xml\n&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;\n&lt;!DOCTYPE properties SYSTEM &quot;http:&#x2F;&#x2F;java.sun.com&#x2F;dtd&#x2F;properties.dtd&quot;&gt;\n&lt;properties&gt;\n        &lt;comment&gt;IK Analyzer 扩展配置&lt;&#x2F;comment&gt;\n        &lt;!--用户可以在这里配置自己的扩展字典 --&gt;\n        &lt;entry key&#x3D;&quot;ext_dict&quot;&gt;my_ext.dic&lt;&#x2F;entry&gt;\n         &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt;\n        &lt;entry key&#x3D;&quot;ext_stopwords&quot;&gt;my_stopword.dic&lt;&#x2F;entry&gt;\n        &lt;!--用户可以在这里配置远程扩展字典 --&gt;\n        &lt;!-- &lt;entry key&#x3D;&quot;remote_ext_dict&quot;&gt;words_location&lt;&#x2F;entry&gt; --&gt;\n        &lt;!--用户可以在这里配置远程扩展停止词字典--&gt;\n        &lt;!-- &lt;entry key&#x3D;&quot;remote_ext_stopwords&quot;&gt;words_location&lt;&#x2F;entry&gt; --&gt;\n&lt;&#x2F;properties&gt;\n[root@linux30 cluster]#\n\n\n2）创建 my_ext.dic 和 my_stopword.dic\n[root@linux30 cluster]# vi node1&#x2F;config&#x2F;analysis-ik&#x2F;my_ext.dic\n## 添加如下内容\n我爱我的祖国\n[root@linux30 cluster]# vi node1&#x2F;config&#x2F;analysis-ik&#x2F;my_stopword.dic\n## 添加如下内容\n祖国\n\n3）只需要重启node1节点即可\n## 分词验证\nGET _analyze\n&#123;\n  &quot;analyzer&quot;: &quot;ik_smart&quot;,\n  &quot;text&quot;: &quot;我爱我的祖国&quot;\n&#125;\n## 返回结果：\n&#123;\n  &quot;tokens&quot; : [\n    &#123;\n      &quot;token&quot; : &quot;我爱我的祖国&quot;,\n      &quot;start_offset&quot; : 0,\n      &quot;end_offset&quot; : 6,\n      &quot;type&quot; : &quot;CN_WORD&quot;,\n      &quot;position&quot; : 0\n    &#125;\n  ]\n&#125;\n\n\n\n4.4 基础查询4.4.1 查询所有酒店信息match_all\n## 查询所有酒店\nGET hotel&#x2F;_search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;match_all&quot;: &#123;&#125;\n  &#125;\n&#125;\n\n4.4.2 分页查询酒店列表from：从哪开始\nsize：查询条数\n## 分页查询酒店列表\nGET hotel&#x2F;_search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;match_all&quot;: &#123;&#125;\n  &#125;,\n  &quot;from&quot;: 0,\n  &quot;size&quot;: 5\n&#125;\n\n4.4.3 品牌精确搜索term：不会对查询条件进行分词\n## 展示出&quot;万豪&quot;品牌下的所有酒店信息\nGET hotel&#x2F;_search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;term&quot;: &#123;\n      &quot;brand&quot;: &quot;万豪&quot;\n    &#125;\n  &#125;\n&#125;\n\n4.4.4 酒店名称分词查询matchQuery会对查询条件进行分词\n## 酒店名称分词查询\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;match&quot;: &#123;\n            &quot;name&quot;: &quot;北京市东城区瑞麟湾酒店&quot;\n        &#125;\n    &#125;\n&#125;\n\n4.4.5 酒店品牌模糊搜索wildcard：不会对查询条件进行分词。还可以使用通配符 ?（任意单个字符） 和  * （0个或多个字符）\n## 酒店品牌模糊搜索\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;wildcard&quot;: &#123;\n            &quot;brand&quot;: &quot;万*&quot;\n        &#125;\n    &#125;\n&#125;\n\n4.4.6 多域搜索queryStringQuery：可以指定多个域、会对搜索条件分词、将分词后的搜索条件与term匹配、取结果并集OR、交集AND\n## 多域搜索\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;query_string&quot;: &#123;\n            &quot;fields&quot;: [&quot;name&quot;, &quot;address&quot;, &quot;area&quot;, &quot;synopsis&quot;],\n            &quot;query&quot;: &quot;spa OR 商务&quot;\n        &#125;\n    &#125;\n&#125;\n\n4.4.7 销量排序搜索 sortorder: asc升序 或 desc降序\n## 销量排序\nGET hotel&#x2F;_search\n&#123;\n    &quot;sort&quot;: &#123;\n        &quot;salesVolume&quot;: &#123;\n            &quot;order&quot;: &quot;asc&quot;\n        &#125;\n    &#125;\n&#125;\n\n4.4.8  价格范围搜索range: gt 大于、gte 大于等于、 lt 小于、lte 小于等于\n## 价格范围搜索\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;range&quot;: &#123;\n            &quot;price&quot;: &#123;\n                &quot;gte&quot;: 600,\n                &quot;lt&quot;: 1600\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n4.4.9 自动纠错搜索fuzzyQuery：自动尝试将条件纠错，并和词条匹配、fuzziness 允许对几个字进行纠错、prefix_length 设置前几个字符不允许编辑\n在未经处理的情况下，一旦条件存在错别字，找不到term，则无法查询到结果\n## 正常搜索\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;term&quot;: &#123;\n           &quot;area&quot;: &quot;北京市&quot;\n       &#125;\n    &#125;\n&#125;\n## 错别字 经\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;term&quot;: &#123;\n           &quot;area&quot;: &quot;北经市&quot;\n       &#125;\n    &#125;\n&#125;\n## 自动纠错搜索 经\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n      &quot;fuzzy&quot;: &#123;\n        &quot;area&quot;: &#123;\n          &quot;fuzziness&quot;: 1,\n          &quot;prefix_length&quot;: 1,\n          &quot;value&quot;: &quot;北经市&quot;\n        &#125;\n      &#125;\n    &#125;\n&#125;\n\n4.4.10 搜索结果高亮显示highlight：如需将搜索条件以高亮形式展示，则需要在查询时，设置需要对哪一个域以何种样式进行展示，fields 设置要对哪个域高亮、pre_tags 设置高亮样式前缀、post_tags 设置高亮样式后缀\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;term&quot;: &#123;\n           &quot;name&quot;: &quot;新乐&quot;\n       &#125;\n    &#125;,\n    &quot;highlight&quot;: &#123;\n        &quot;fields&quot;: &#123;\n            &quot;name&quot;: &#123;\n                &quot;pre_tags&quot;: &quot;&lt;font color&#x3D;&#39;red&#39;&gt;&quot;,\n                &quot;post_tags&quot;: &quot;&lt;&#x2F;font&gt;&quot;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n4.5 bool查询boolQuery:  对多个查询条件拼接、must（and）条件必须成立、must_not （not）条件必须不成立、should（or）条件可以成立、filter 条件过滤，必须成立\n4.5.1 must## must单独使用 品牌必须是万豪，地区必须是北京市\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;bool&quot;: &#123;\n            &quot;must&quot;: [\n                &#123;\n                   &quot;term&quot;: &#123;\n                        &quot;brand&quot;: &quot;万豪&quot;\n                    &#125;\n                &#125;,&#123;\n                    &quot;term&quot;: &#123;\n                        &quot;area&quot;: &quot;北京市&quot;\n                    &#125;\n                &#125;\n            ]\n        &#125;\n    &#125;\n&#125;\n\n4.5.2 filter## filter单独使用 \nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;bool&quot;: &#123;\n            &quot;filter&quot;: [\n                &#123;\n                    &quot;term&quot;: &#123;\n                         &quot;brand&quot;: &quot;万豪&quot;\n                    &#125;\n                &#125;,&#123;\n                    &quot;term&quot;: &#123;\n                        &quot;area&quot;: &quot;北京市&quot;\n                    &#125;\n                &#125;\n            ]\n        &#125;\n    &#125;\n&#125;\n\n4.5.3 must和filter组合使用## must和filter组合使用 品牌为万豪下的，地区为北京市、价格范围在500和2000之间的酒店\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;bool&quot;: &#123;\n            &quot;must&quot;: [\n                &#123;\n                   &quot;term&quot;: &#123;\n                        &quot;brand&quot;: &quot;万豪&quot;\n                    &#125;\n                &#125;\n            ],\n            &quot;filter&quot;: [\n                &#123;\n                    &quot;term&quot;: &#123;\n                        &quot;area&quot;: &quot;北京市&quot;\n                    &#125;\n                &#125;,&#123;\n                    &quot;range&quot;: &#123;\n                        &quot;price&quot;: &#123;\n                            &quot;gte&quot;: 500,\n                            &quot;lte&quot;: 2000\n                        &#125;\n                    &#125;\n                &#125;\n            ]\n        &#125;\n    &#125;\n&#125;\n\n4.6 聚合查询聚合介绍：在MySQL中提供了许多聚合函数，如max、min、avg、count等。并且也提供了分组实现group by。对于这些功能，在es中同样提供，主要用于对数据统计分析。\nES中的聚合搜索分为两类：指标聚合、桶聚合\n指标聚合：如max、min、sum等。作用等同MySQL中相关聚合函数\n桶聚合：用于数据分组，作用等同于MySQL中的group by。\nps：不能对Text类型分组，因为会分词。\n4.6.1 指标聚合## 统计品牌为万豪下最贵酒店价格\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;term&quot;: &#123;\n           &quot;brand&quot;: &quot;万豪&quot;\n       &#125;\n    &#125;,\n    &quot;aggs&quot;: &#123;\n        &quot;my_max_price&quot;: &#123;\n            &quot;max&quot;: &#123;\n                &quot;field&quot;: &quot;price&quot;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n4.6.2 桶聚合1）统计品牌为万豪下有哪些星级\n## 统计品牌为万豪下有哪些星级\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;term&quot;: &#123;\n           &quot;brand&quot;: &quot;万豪&quot;\n       &#125;\n    &#125;,\n    &quot;aggs&quot;: &#123;\n        &quot;my_group&quot;: &#123;\n            &quot;terms&quot;: &#123;\n                &quot;field&quot;: &quot;specs&quot;,\n                &quot;size&quot;: 5\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n2）根据搜索条件对品牌分组\n## 根据搜索条件对品牌分组\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;query_string&quot;: &#123;\n           &quot;fields&quot;: [&quot;name&quot;, &quot;synopsis&quot;, &quot;area&quot;, &quot;address&quot;],\n           &quot;query&quot;: &quot;三亚 OR 商务&quot;\n       &#125;\n    &#125;,\n    &quot;aggs&quot;: &#123;\n        &quot;hotel_brand&quot;: &#123;\n            &quot;terms&quot;: &#123;\n                &quot;field&quot;: &quot;brand&quot;,\n                &quot;size&quot;: 100\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n4.6.3 示例## 自定义时间段统计某品牌下酒店销量\nGET hotel&#x2F;_search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;range&quot;: &#123;\n      &quot;createTime&quot;: &#123;\n        &quot;gte&quot;: &quot;2016-01-01&quot;,\n        &quot;lte&quot;: &quot;2021-01-01&quot;\n      &#125;\n    &#125;\n  &#125;,\n  &quot;aggs&quot;: &#123;\n    &quot;hotel_brand&quot;: &#123;\n      &quot;terms&quot;: &#123;\n        &quot;field&quot;: &quot;brand&quot;,\n        &quot;size&quot;: 100\n      &#125;,\n      &quot;aggs&quot;: &#123;\n        &quot;sale_count&quot;: &#123;\n          &quot;sum&quot;: &#123;\n            &quot;field&quot;: &quot;salesVolume&quot;\n          &#125;\n        &#125;\n      &#125;\n    &#125;\n  &#125;\n&#125;\n\n\n\n5. ElasticSearch实战技巧5.1 优化多字段查询搜索时，对于每条搜索结果ES都会对其按照匹配度进行打分，分数越高，在结果中排名越靠前。\n在ES中提供了两种设置权重的方式：索引设置、查询设置。\n索引设置：创建索引时配置权重，该方式应用较少，因为一旦需求发生改变，则需要重新创建索引。\n查询设置：在查询时，根据需求灵活的配置权重，该方式使用最多。\n5.1.1 提升字段查询得分将name字段查询比重提升10倍\nGET hotel&#x2F;_search\n&#123;\n  &quot;explain&quot;: true, \n  &quot;query&quot;:&#123;\n    &quot;multi_match&quot;:&#123;\n      &quot;query&quot;: &quot;北京金龙&quot;,\n      &quot;fields&quot;: [&quot;name^10&quot;, &quot;address&quot;]\n    &#125;\n  &#125;\n&#125;\n\n5.1.2 综合提升字段查询得分tie_breaker：将其他query的分数也考虑进去（最大值加上其他值的0.3倍）\nGET hotel&#x2F;_search\n&#123;\n  &quot;explain&quot;: true, \n  &quot;query&quot;:&#123;\n    &quot;multi_match&quot;:&#123;\n      &quot;query&quot;: &quot;北京金龙&quot;,\n      &quot;fields&quot;: [&quot;name&quot;, &quot;address&quot;],\n      &quot;tie_breaker&quot;: 0.3\n    &#125;\n  &#125;\n&#125;\n\n使用 tie_breaker 和不使用tie_breaker ，查询出来的某一条数据的 _score 分数，会有相应的提高，例如：\n\n\n\n\n\n\n\n\n\nname中包含关键词matched query 的得分，假设是 0.1984226\naddress中包含关键词matched query的得分，假设是 12.07466\n添加了 tie_breaker = 0.3，那么就是这样的了， 0.1984226 * 0.3 + 12.07466 = 12.13418678；\n大于最高一条的得分12.07466，这样搜索的关联性就提升上去了， 更为合理。\n5.1.3 自定义评分1）创建索引时设置权重\n## 查询多域展示相关结果数据\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;query_string&quot;: &#123;\n           &quot;fields&quot;: [&quot;name&quot;, &quot;synopsis&quot;, &quot;area&quot;, &quot;address&quot;],\n           &quot;query&quot;: &quot;北京市万豪spa三星&quot;\n       &#125;\n    &#125;\n&#125;\n\n## 评分扩大10倍\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;query_string&quot;: &#123;\n           &quot;fields&quot;: [&quot;name&quot;, &quot;synopsis&quot;, &quot;area&quot;, &quot;address&quot;],\n           &quot;query&quot;: &quot;北京市万豪spa三星&quot;,\n           &quot;boost&quot;: 10\n       &#125;\n    &#125;\n&#125;\n\n2）查询设置权重\nfunction_score\n## 为品牌为万豪的酒店，权重值增加50倍\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n       &quot;function_score&quot;: &#123;\n           &quot;query&quot;: &#123;\n               &quot;query_string&quot;: &#123;\n                    &quot;fields&quot;: [&quot;name&quot;, &quot;synopsis&quot;, &quot;area&quot;, &quot;address&quot;],\n                     &quot;query&quot;: &quot;北京市万豪spa三星&quot;\n               &#125;\n           &#125;,\n           &quot;functions&quot;: [\n               &#123;\n                   &quot;filter&quot;: &#123;\n                       &quot;term&quot;: &#123;\n                           &quot;brand&quot;: &quot;万豪&quot;\n                       &#125;\n                   &#125;,\n                   &quot;weight&quot;: 50\n               &#125;\n           ]\n       &#125;\n    &#125;\n&#125;\n## 将广告酒店的权重增加100倍，使其靠前\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;function_score&quot;: &#123;\n            &quot;query&quot;: &#123;\n                &quot;query_string&quot;: &#123;\n                    &quot;fields&quot;: [&quot;name&quot;, &quot;synopsis&quot;, &quot;area&quot;, &quot;address&quot;],\n                    &quot;query&quot;: &quot;北京市万豪spa三星&quot;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\nGET hotel&#x2F;_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;function_score&quot;: &#123;\n            &quot;query&quot;: &#123;\n                &quot;query_string&quot;: &#123;\n                    &quot;fields&quot;: [&quot;name&quot;, &quot;synopsis&quot;, &quot;area&quot;, &quot;address&quot;],\n                    &quot;query&quot;: &quot;北京市万豪spa三星&quot;\n                &#125;\n            &#125;,\n            &quot;functions&quot;: [\n                &#123;\n                    &quot;filter&quot;: &#123;\n                        &quot;term&quot;: &#123;\n                            &quot;isAd&quot;: &quot;1&quot;\n                        &#125;\n                    &#125;,\n                    &quot;weight&quot;: 100\n                &#125;\n            ]\n        &#125;\n    &#125;\n&#125;\n\n5.2 全量索引构建5.21 下载logstash下载地址：https://artifacts.elastic.co/downloads/logstash/logstash-7.10.2-linux-x86_64.tar.gz\n5.2.2 安装logstash-input-jdbc插件bin&#x2F;logstash-plugin install logstash-input-jdbc\n\n5.2.3 配置mysql驱动包[root@linux30 logstash-7.10.2]# mkdir mysql\n[root@linux30 logstash-7.10.2]# cd mysql&#x2F;\n[root@linux30 mysql]# ll\n总用量 2344\n-rw-r--r-- 1 root root 2397321 4月  11 14:12 mysql-connector-java-8.0.21.jar\n\n5.2.4 配置JDBC连接创建索引数据是从mysql中通过select语句查询， 然后再通过logstash-input-jdbc的配置文件方式导入 elasticsearch中。\n在/opt/elasticsearch/logstash-7.10.2/mysql/full-sync目录创建jdbc.conf与jdbc.sql文件。\njdbc.conf文件：\ninput &#123;\n    stdin &#123;\n    &#125;\n    jdbc &#123;\n        # mysql 数据库链接,users为数据库名\n        jdbc_connection_string &#x3D;&gt; &quot;jdbc:mysql:&#x2F;&#x2F;192.168.10.30:3306&#x2F;elasticsearch_db&quot;\n        # 用户名和密码\n        jdbc_user &#x3D;&gt; &quot;root&quot;\n        jdbc_password &#x3D;&gt; &quot;123456&quot;\n        # 驱动\n        jdbc_driver_library &#x3D;&gt; &quot;&#x2F;opt&#x2F;elasticsearch&#x2F;logstash-7.10.2&#x2F;mysql&#x2F;mysql-connector-java-8.0.21.jar&quot;\n        # 驱动类名\n        jdbc_driver_class &#x3D;&gt; &quot;com.mysql.cj.jdbc.Driver&quot;\n        jdbc_paging_enabled &#x3D;&gt; &quot;true&quot;\n        jdbc_page_size &#x3D;&gt; &quot;50000&quot;\n        # 执行的sql 文件路径+名称\n        statement_filepath &#x3D;&gt; &quot;&#x2F;opt&#x2F;elasticsearch&#x2F;logstash-7.10.2&#x2F;mysql&#x2F;full-sync&#x2F;jdbc.sql&quot;\n        # 设置监听间隔 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新\n        schedule &#x3D;&gt; &quot;* * * * *&quot;\n    &#125;\n&#125;\n\noutput &#123;\n        elasticsearch &#123;\n                #ES的连接信息\n                hosts &#x3D;&gt; [&quot;192.168.10.30:9200&quot;]\n                #索引名称\n                index &#x3D;&gt; &quot;hotel&quot;\n                document_type &#x3D;&gt; &quot;_doc&quot;\n                #自增ID， 需要关联的数据库的ID字段， 对应索引的ID标识\n                document_id &#x3D;&gt; &quot;%&#123;id&#125;&quot;\n        &#125;\n        stdout &#123;\n                #JSON格式输出\n                codec &#x3D;&gt; json_lines\n        &#125;\n&#125;\n\n\n\njdbc.sql文件：\nSELECT\n        id,\n        NAME,\n        address,\n        brand,\n        type,\n        price,\n        specs,\n        salesVolume,\n        synopsis,\n        area,\n        imageUrl,\n        createTime,\n        isAd\nFROM\n        t_hotel\n\n5.2.5 执行全量同步[root@linux30 full-sync]# ..&#x2F;..&#x2F;bin&#x2F;logstash -f jdbc.conf\n\n\n5.2.6 检查结果GET hotel&#x2F;_search\n\n5.3 增量索引同步5.3.1 修改jdbc.conf配置文件input &#123;\n    stdin &#123;\n    &#125;\n    jdbc &#123;\n        # mysql 数据库链接,users为数据库名\n        jdbc_connection_string &#x3D;&gt; &quot;jdbc:mysql:&#x2F;&#x2F;192.168.10.30:3306&#x2F;elasticsearch_db&quot;\n        # 用户名和密码\n        jdbc_user &#x3D;&gt; &quot;root&quot;\n        jdbc_password &#x3D;&gt; &quot;123456&quot;\n        # 驱动\n        jdbc_driver_library &#x3D;&gt; &quot;&#x2F;opt&#x2F;elasticsearch&#x2F;logstash-7.10.2&#x2F;mysql&#x2F;mysql-connector-java-8.0.21.jar&quot;\n        # 驱动类名\n        jdbc_driver_class &#x3D;&gt; &quot;com.mysql.cj.jdbc.Driver&quot;\n        jdbc_paging_enabled &#x3D;&gt; &quot;true&quot;\n        jdbc_page_size &#x3D;&gt; &quot;50000&quot;\n        # 执行的sql 文件路径+名称\n        statement_filepath &#x3D;&gt; &quot;&#x2F;opt&#x2F;elasticsearch&#x2F;logstash-7.10.2&#x2F;mysql&#x2F;inc-sync&#x2F;jdbc.sql&quot;\n        # 设置监听间隔 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新\n        schedule &#x3D;&gt; &quot;* * * * *&quot;\n        #设置timezone\n        jdbc_default_timezone &#x3D;&gt; &quot;Asia&#x2F;Shanghai&quot;\n        # 增量同步属性标识\n        last_run_metadata_path &#x3D;&gt; &quot;&#x2F;opt&#x2F;elasticsearch&#x2F;logstash-7.10.2&#x2F;mysql&#x2F;inc-sync&#x2F;last_value&quot;\n    &#125;\n&#125;\n\noutput &#123;\n        elasticsearch &#123;\n                #ES的连接信息\n                hosts &#x3D;&gt; [&quot;192.168.10.30:9200&quot;]\n                #索引名称\n                index &#x3D;&gt; &quot;hotel&quot;\n                document_type &#x3D;&gt; &quot;_doc&quot;\n                #自增ID， 需要关联的数据库的ID字段， 对应索引的ID标识\n                document_id &#x3D;&gt; &quot;%&#123;id&#125;&quot;\n        &#125;\n        stdout &#123;\n                #JSON格式输出\n                codec &#x3D;&gt; json_lines\n        &#125;\n&#125;\n\n\n5.3.2 修改jdbc.sql配置文件SELECT\n        id,\n        NAME,\n        address,\n        brand,\n        type,\n        price,\n        specs,\n        salesVolume,\n        synopsis,\n        area,\n        imageUrl,\n        createTime,\n        isAd\nFROM\n        t_hotel\nWHERE\n        createTime &gt;&#x3D; :sql_last_value\n        \n        \n        \n\n\n5.3.3 创建同步最后记录时间vi &#x2F;opt&#x2F;elasticsearch&#x2F;logstash-7.10.2&#x2F;mysql&#x2F;inc-sync&#x2F;last_value\n\n给定一个初始的时间：\n2022-04-12 00:00:00\n\n5.3.4 验证启动logstash， 会根据初始时间，加载对应的数据。\n如果修改的数据的更新时间， 会自动检测， 同步增量的数据。\nMySQL [elasticsearch_db]&gt; insert into elasticsearch_db.t_hotel (name, address, brand, type, price, specs, salesVolume, synopsis, area, imageUrl, createTime, isAd) values (&quot;test hotel&quot;,&quot;test address&quot;, &quot;test brand&quot;, &quot;test type&quot;, 100, &quot;test specs&quot;, 100, &quot;test data&quot;, &quot;test data&quot;, &quot;www.baidu.com&quot;, &quot;2022-06-17&quot;, 1);\nQuery OK, 1 row affected, 1 warning (0.01 sec)\n\n\nGET hotel&#x2F;_search\n&#123;\n  &quot;query&quot;: &#123;\n    &quot;match&quot;: &#123;\n      &quot;name&quot;: &quot;test hotel&quot;\n    &#125;\n  &#125;\n&#125;\n","slug":"Es原理与使用","date":"2022-11-18T01:42:22.000Z","categories_index":"","tags_index":"es","author_index":"Aurora"},{"id":"12a033ee2fac773562e18cc831186809","title":"计算机原理","content":"计算机体系结构1、CPU（中央处理器）1.1、概念：负责提取程序指令，并对指令进行编码，然后按着程序规定的顺序对正确的数据执行各种操作\n\n1.2、组成：中央处理器可以分为两部分：数据通道和控制单元\n\n1.2.1、数据通道1.2.1.1、寄存器寄存器是一种存储二进制数据的硬件设备,寄存器位于处理器的内部，所以处理器可以非常快的访问寄存器中存储的各种信息；计算机通常处理的是一些存储在寄存器中，具有固定大小的二进制的数据；寄存器通常配置是2的指数幂。\n可以向寄存器写入或读取信息，信息也可以在不同的寄存之间传递。寄存器的编址方式与存储器不同，每个存储器都有一个唯一的二进制地址，这些地址是从0开始编码；寄存器则是又cpu内部的控制单元进行编址和处理的。\n\n1.2.1.2、算数逻辑单元（ALU）在程序执行过程中进行逻辑运算（如比较运算）和算数运算（如加法和减法）。\nALU的各种操作常常会影响状态寄存器的某些数据位的数值（设置这些数据位是为了指示某些动作，例如是否有溢出发生）。通过控制单元发出的信号，控制ALU执行各种规定的运算。\n\n1.2.2、控制单元控制单元负责监视所有指令的执行和各种信息的传送过程。控制单元负责从内存提取指令，对这些指令进行译码，确保数据适时的出现在正确的地方。控制单元还负责通知ALU应该使用那个寄存器，执行那些中断服务程序，以及对所需执行的各种操作接通ALU中的正确电路，控制单元使用一个称为程序计数器的寄存器来寻找下一条要执行的指令位置，并使用一个状态寄存器来存放某些特殊的状态。\n\n2.2、总线CPU通过总线与其他部件连接起来。总线是一组导电线路组合，他作为一个共享和公用的的数据通道将系统内的各个子系统连接在一起。总线由多条线路构成，这样的总线允许多位数据并行传递，\n\n2.3、时钟每台计算机都有一个内部时钟，用来控制指令的执行速度。时钟也是用来对系统各个部件的操作进行协调和同步。\n\n2.4、输入/输出子系统\n\n","slug":"计算机原理","date":"2022-11-11T02:48:54.000Z","categories_index":"","tags_index":"","author_index":"Aurora"}]